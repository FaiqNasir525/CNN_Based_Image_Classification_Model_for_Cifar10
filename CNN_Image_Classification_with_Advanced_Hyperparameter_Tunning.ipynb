{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaiqNasir525/Custom_CNN_Based_Image_Classification_Model/blob/main/CNN_Image_Classification_with_Advanced_Hyperparameter_Tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "-G1mZVGHdzsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3zIWF80K3oBt",
        "outputId": "e7ea66df-81f5-4e58-9ffa-b8cbbbfd9efa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras_tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preprocessing Data"
      ],
      "metadata": {
        "id": "riFQMeFId6Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "#Normalizing from 0 to 1 Range\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "#Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "train_generator = datagen.flow(train_images, train_labels, batch_size=64)\n",
        "\n",
        "#Writing class names fto better visulization\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "0KHoMr77c8c_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Dynamic and Hyper-Tunable Model"
      ],
      "metadata": {
        "id": "7Qw3Ngq4d_Uc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qlL4bP7-jgvD"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define a function to build the model.\n",
        "def build_model(hp):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Tune the number of convolutional layers (1, 2 or 3)\n",
        "    for i in range(hp.Int('conv_layers', 1, 3)):\n",
        "        if i == 0:\n",
        "            model.add(layers.Conv2D(\n",
        "                filters=hp.Int('filters_' + str(i), min_value=32, max_value=128, step=16),\n",
        "                kernel_size=3,\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(1e-4),\n",
        "                padding='same',\n",
        "                input_shape=(32, 32, 3)))\n",
        "        else:\n",
        "            for j in range(hp.Int('deep_conv_layers', 1, 3)):\n",
        "                model.add(layers.Conv2D(\n",
        "                    filters=hp.Int('filters_' + str(i) + str(j), min_value=64, max_value=128, step=16),\n",
        "                    kernel_size=3,\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(1e-4),\n",
        "                    padding='same'))\n",
        "                model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(layers.Dropout(rate=hp.Float(f'dropoutConv_{i}', 0.0, 0.5, step=0.1)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Tune the number of dense layers (1, 2, or 3)\n",
        "    for i in range(hp.Int('dense_layers', 1, 3)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=32, max_value=128, step=16),\n",
        "            activation='relu'))\n",
        "\n",
        "        # Tune the dropout rate\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(rate=hp.Float(f'dropoutDense_{i}', 0.0, 0.5, step=0.1)))\n",
        "\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Choose an optimizer and learning rate\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining and Running the Tuner"
      ],
      "metadata": {
        "id": "HmP5KEOYePlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='cifar10_tunning'\n",
        ")\n",
        "\n",
        "# Step 6: Perform the Hyperparameter search\n",
        "tuner.search(train_generator, batch_size=128, epochs=15, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Step 7: Get the best Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "D4AGo6QndJtk",
        "outputId": "e05e5399-4699-4b76-9f13-19b78064b32a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 09m 35s]\n",
            "val_loss: 1.223215937614441\n",
            "\n",
            "Best val_loss So Far: 0.8188191652297974\n",
            "Total elapsed time: 01h 41m 52s\n",
            "\n",
            "Search: Running Trial #11\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "3                 |3                 |conv_layers\n",
            "80                |32                |filters_0\n",
            "0.4               |0.2               |dropoutConv_0\n",
            "2                 |1                 |dense_layers\n",
            "128               |32                |units_0\n",
            "0.3               |0                 |dropoutDense_0\n",
            "0.0001            |0.001             |learning_rate\n",
            "3                 |2                 |deep_conv_layers\n",
            "112               |64                |filters_10\n",
            "0                 |0.2               |dropoutConv_1\n",
            "32                |32                |units_1\n",
            "0.2               |0.4               |dropoutDense_1\n",
            "112               |112               |filters_11\n",
            "80                |64                |filters_12\n",
            "64                |112               |filters_20\n",
            "80                |80                |filters_21\n",
            "96                |80                |filters_22\n",
            "0                 |0.3               |dropoutConv_2\n",
            "32                |128               |units_2\n",
            "0.3               |0.2               |dropoutDense_2\n",
            "\n",
            "Epoch 1/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 64ms/step - accuracy: 0.1755 - loss: 2.6056 - val_accuracy: 0.2743 - val_loss: 2.2468\n",
            "Epoch 2/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 49ms/step - accuracy: 0.2855 - loss: 2.0492 - val_accuracy: 0.2894 - val_loss: 2.1673\n",
            "Epoch 3/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.3497 - loss: 1.8689 - val_accuracy: 0.3035 - val_loss: 2.2926\n",
            "Epoch 4/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.3892 - loss: 1.7547 - val_accuracy: 0.2860 - val_loss: 2.7914\n",
            "Epoch 5/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.4268 - loss: 1.6476 - val_accuracy: 0.3330 - val_loss: 2.3174\n",
            "Epoch 6/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.4590 - loss: 1.5546 - val_accuracy: 0.4901 - val_loss: 1.5594\n",
            "Epoch 7/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.4894 - loss: 1.4877 - val_accuracy: 0.4506 - val_loss: 1.7591\n",
            "Epoch 8/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - accuracy: 0.5157 - loss: 1.4242 - val_accuracy: 0.4566 - val_loss: 1.8292\n",
            "Epoch 9/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.5334 - loss: 1.3790 - val_accuracy: 0.5419 - val_loss: 1.3786\n",
            "Epoch 10/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.5476 - loss: 1.3338 - val_accuracy: 0.5625 - val_loss: 1.3160\n",
            "Epoch 11/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 51ms/step - accuracy: 0.5710 - loss: 1.2844 - val_accuracy: 0.5590 - val_loss: 1.3756\n",
            "Epoch 12/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - accuracy: 0.5820 - loss: 1.2539 - val_accuracy: 0.5028 - val_loss: 1.5846\n",
            "Epoch 13/15\n",
            "\u001b[1m148/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.6009 - loss: 1.1986"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing the best Parameters"
      ],
      "metadata": {
        "id": "V8-kE5QGeo6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Print the all the best hyperparameters\n",
        "print(f\"Best number of convolutional layers: {best_hps.get('conv_layers')}\")\n",
        "print(f\"Best number of deep convolutional layers: {best_hps.get('deep_conv_layers')}\")\n",
        "print(f\"Best number of dense layers: {best_hps.get('dense_layers')}\")\n",
        "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "for i in range(best_hps.get('conv_layers')):\n",
        "  print(f\"Best number of filters for the convolutional layer {i+1}: {best_hps.get(f'filters_{i}')}\")\n",
        "  print(f\"Best dropout rate for the convolutional layer {i+1}: {best_hps.get(f'dropoutConv_{i}')}\")\n",
        "\n",
        "  if best_hps.get('deep_conv_layers') > 1:\n",
        "    for j in range(best_hps.get('deep_conv_layers')):\n",
        "      print(f\"Best number of filters for the deep convolutional layer {i+1}.{j+1}: {best_hps.get(f'filters_{i}{j}')}\")\n",
        "\n",
        "for i in range(best_hps.get('dense_layers')):\n",
        "  print(f\"Best number of units for the dense layer {i+1}: {best_hps.get(f'units_{i}')}\")\n",
        "  print(f\"Best dropout rate for the dense layer {i+1}: {best_hps.get(f'dropout_{i}')}\")\n",
        "\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_JHqm11njbZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting and Training on Best Model"
      ],
      "metadata": {
        "id": "nV9S51gteyt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Build the model with the best Hyperparameters and train it\n",
        "early_stopping = EarlyStopping(\n",
        "       monitor='val_loss',  # Metric to monitor\n",
        "       patience=3,          # Number of epochs with no improvement before stopping\n",
        "       restore_best_weights=True  # Restore the weights of the best epoch\n",
        "   )\n",
        "# Train the model\n",
        "history = model.fit(train_generator, batch_size=64, epochs=100, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "KdBmFKLCdXCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Accuracy and Loss"
      ],
      "metadata": {
        "id": "-gpjZ5WUe4L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Plotting training & validation accuracy and loss values\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53OfWGhcddbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Model"
      ],
      "metadata": {
        "id": "CTRYmTZke-Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing on the Training Data\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=4)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "VE8cHCfxCdj7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}