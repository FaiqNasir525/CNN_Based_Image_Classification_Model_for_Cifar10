{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaiqNasir525/Custom_CNN_Based_Image_Classification_Model/blob/main/CNN_Image_Classification_with_Advanced_Hyperparameter_Tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "-G1mZVGHdzsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3zIWF80K3oBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preprocessing Data"
      ],
      "metadata": {
        "id": "riFQMeFId6Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "#Normalizing from 0 to 1 Range\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "#Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "train_generator = datagen.flow(train_images, train_labels, batch_size=64)\n",
        "\n",
        "#Writing class names fto better visulization\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "0KHoMr77c8c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Dynamic and Hyper-Tunable Model"
      ],
      "metadata": {
        "id": "7Qw3Ngq4d_Uc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlL4bP7-jgvD"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define a function to build the model.\n",
        "def build_model(hp):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Tune the number of convolutional layers (1, 2 or 3)\n",
        "    for i in range(hp.Int('conv_layers', 1, 3)):\n",
        "        if i == 0:\n",
        "            model.add(layers.Conv2D(\n",
        "                filters=hp.Int('filters_' + str(i), min_value=32, max_value=128, step=16),\n",
        "                kernel_size=3,\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(1e-4),\n",
        "                padding='same',\n",
        "                input_shape=(32, 32, 3)))\n",
        "        else:\n",
        "            for j in range(hp.Int('deep_conv_layers', 1, 3)):\n",
        "                model.add(layers.Conv2D(\n",
        "                    filters=hp.Int('filters_' + str(i) + str(j), min_value=64, max_value=128, step=16),\n",
        "                    kernel_size=3,\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(1e-4),\n",
        "                    padding='same'))\n",
        "                model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(layers.Dropout(rate=hp.Float(f'dropoutConv_{i}', 0.0, 0.5, step=0.1)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Tune the number of dense layers (1, 2, or 3)\n",
        "    for i in range(hp.Int('dense_layers', 1, 3)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=32, max_value=128, step=16),\n",
        "            activation='relu'))\n",
        "\n",
        "        # Tune the dropout rate\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(rate=hp.Float(f'dropoutDense_{i}', 0.0, 0.5, step=0.1)))\n",
        "\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Choose an optimizer and learning rate\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining and Running the Tuner"
      ],
      "metadata": {
        "id": "HmP5KEOYePlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the Tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='cifar10_tunning'\n",
        ")\n",
        "\n",
        "# Step 6: Perform the Hyperparameter search\n",
        "tuner.search(train_images, train_labels, batch_size=64, epochs=15, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Step 7: Get the best Hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "D4AGo6QndJtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing the best Parameters"
      ],
      "metadata": {
        "id": "V8-kE5QGeo6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Print the all the best hyperparameters\n",
        "print(f\"Best number of convolutional layers: {best_hps.get('conv_layers')}\")\n",
        "print(f\"Best number of deep convolutional layers: {best_hps.get('deep_conv_layers')}\")\n",
        "print(f\"Best number of dense layers: {best_hps.get('dense_layers')}\")\n",
        "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "for i in range(best_hps.get('conv_layers')):\n",
        "  print(f\"Best number of filters for the convolutional layer {i+1}: {best_hps.get(f'filters_{i}')}\")\n",
        "  print(f\"Best dropout rate for the convolutional layer {i+1}: {best_hps.get(f'dropoutConv_{i}')}\")\n",
        "\n",
        "  if best_hps.get('deep_conv_layers') > 1:\n",
        "    for j in range(best_hps.get('deep_conv_layers')):\n",
        "      print(f\"Best number of filters for the deep convolutional layer {i+1}.{j+1}: {best_hps.get(f'filters_{i}{j}')}\")\n",
        "\n",
        "for i in range(best_hps.get('dense_layers')):\n",
        "  print(f\"Best number of units for the dense layer {i+1}: {best_hps.get(f'units_{i}')}\")\n",
        "  print(f\"Best dropout rate for the dense layer {i+1}: {best_hps.get(f'dropout_{i}')}\")"
      ],
      "metadata": {
        "id": "_JHqm11njbZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting and Training on Best Model"
      ],
      "metadata": {
        "id": "nV9S51gteyt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Build the model with the best Hyperparameters and train it\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "early_stopping = EarlyStopping(\n",
        "       monitor='val_loss',  # Metric to monitor\n",
        "       patience=3,          # Number of epochs with no improvement before stopping\n",
        "       restore_best_weights=True  # Restore the weights of the best epoch\n",
        "   )\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, batch_size=64, epochs=100, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "KdBmFKLCdXCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Accuracy and Loss"
      ],
      "metadata": {
        "id": "-gpjZ5WUe4L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Plotting training & validation accuracy and loss values\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53OfWGhcddbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Model"
      ],
      "metadata": {
        "id": "CTRYmTZke-Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing on the Training Data\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=4)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE8cHCfxCdj7",
        "outputId": "d296c3ca-b107-48b6-d0cd-b7e8ebf167df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - 3ms/step - accuracy: 0.8172 - loss: 0.6956\n",
            "0.8172000050544739\n"
          ]
        }
      ]
    }
  ]
}